{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d81eedb5-9347-4e04-9f7c-55494c26b5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Reading raw data...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 1. Spark Setup and Data Reading\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Create a SparkSession for the application\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"AmazonReviewBalancedClassifier\").getOrCreate()\n",
    "\n",
    "print(\"\\nüì• Reading raw data...\")\n",
    "# Load the Amazon review dataset (JSON format)\n",
    "raw_df = spark.read.json(r\"C:\\Users\\AYOUB KHABALI\\Documents\\GitHub\\Amazon_Models_training-\\code\\train_data.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75ebccef-e514-4a7f-a391-765ff250f805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üßæ Original class distribution:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  420|\n",
      "|    1|  695|\n",
      "|    2| 8119|\n",
      "+-----+-----+\n",
      "\n",
      "\n",
      "üìù Average review length by class:\n",
      "+-----+------------------+\n",
      "|label|  avg(text_length)|\n",
      "+-----+------------------+\n",
      "|    0| 609.7857142857143|\n",
      "|    1| 579.6086330935252|\n",
      "|    2|483.21246458923514|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 2. Data Preprocessing\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import col, when, concat_ws, lit, trim, length, avg, count\n",
    "\n",
    "# Combine 'summary' and 'reviewText' into a single 'text' column\n",
    "df = raw_df.withColumn(\"text\", \n",
    "    concat_ws(\" \", \n",
    "        when(col(\"summary\").isNull(), \"\").otherwise(col(\"summary\")),\n",
    "        when(col(\"reviewText\").isNull(), \"\").otherwise(col(\"reviewText\"))\n",
    "    ))\n",
    "\n",
    "# Replace null or empty reviews with the placeholder \"empty_review\"\n",
    "df = df.withColumn(\"text\", \n",
    "    when(col(\"text\").isNull() | (trim(col(\"text\")) == \"\"), lit(\"empty_review\")).otherwise(col(\"text\")))\n",
    "\n",
    "# Display the class distribution\n",
    "print(\"\\nüßæ Original class distribution:\")\n",
    "df.groupBy(\"label\").agg(count(\"*\").alias(\"count\")).orderBy(\"label\").show()\n",
    "\n",
    "# Display average review length per class\n",
    "print(\"\\nüìù Average review length by class:\")\n",
    "df.withColumn(\"text_length\", length(col(\"text\"))).groupBy(\"label\").agg(avg(\"text_length\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0829552b-ab73-45a3-adae-20389a6f719a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Balancing dataset...\n",
      "\n",
      "‚úÖ Balanced class distribution:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0| 1500|\n",
      "|    1| 1500|\n",
      "|    2| 1500|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 3. Balancing the Dataset\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Define the target number of samples per class\n",
    "target_size = 1500\n",
    "\n",
    "# Filter data by class\n",
    "class_0 = df.filter(col(\"label\") == 0)\n",
    "class_1 = df.filter(col(\"label\") == 1)\n",
    "class_2 = df.filter(col(\"label\") == 2)\n",
    "\n",
    "# Oversampling function for balancing minority classes\n",
    "def oversample(df_class, target):\n",
    "    ratio = int(target / df_class.count()) + 1\n",
    "    oversampled = df_class\n",
    "    for _ in range(ratio - 1):\n",
    "        oversampled = oversampled.union(df_class)\n",
    "    return oversampled.orderBy(rand()).limit(target)\n",
    "\n",
    "print(\"\\nüîÅ Balancing dataset...\")\n",
    "\n",
    "# Apply oversampling or downsampling to balance classes\n",
    "balanced_0 = oversample(class_0, target_size)\n",
    "balanced_1 = oversample(class_1, target_size)\n",
    "balanced_2 = class_2.orderBy(rand()).limit(target_size)\n",
    "\n",
    "# Merge and shuffle the balanced dataset\n",
    "balanced_df = balanced_0.union(balanced_1).union(balanced_2).orderBy(rand())\n",
    "\n",
    "# Display the new balanced class distribution\n",
    "print(\"\\n‚úÖ Balanced class distribution:\")\n",
    "balanced_df.groupBy(\"label\").agg(count(\"*\").alias(\"count\")).orderBy(\"label\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "675ff224-f33f-4562-ab3a-9486adc98334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 4. Pipeline: Tokenization ‚Üí Stopword Removal ‚Üí Vectorization\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Tokenize the text column into words\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "\n",
    "# Remove stopwords\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "\n",
    "# Convert words into a vector of token counts\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"rawFeatures\", vocabSize=10000, minDF=2)\n",
    "\n",
    "# Compute the Inverse Document Frequency (IDF)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "\n",
    "# Convert string labels into indexed labels\n",
    "indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "\n",
    "# Define a Naive Bayes classifier\n",
    "nb = NaiveBayes(featuresCol=\"features\", labelCol=\"label_index\", modelType=\"multinomial\", smoothing=2.5)\n",
    "\n",
    "# Combine all steps into a single ML pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords_remover, vectorizer, idf, indexer, nb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "09b61482-9d56-4869-b992-512393aebcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÇ Starting training...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5. Train-Test Split and Training\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nüöÇ Starting training...\")\n",
    "\n",
    "# Split the balanced dataset into training (90%) and test (10%)\n",
    "train_data, test_data = balanced_df.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# Train the model using the pipeline\n",
    "model = pipeline.fit(train_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d43ddf8-f37d-4d52-99ea-c5c0c5830706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Running prediction on test data...\n",
      "\n",
      "=== Classification Report ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.81      0.81       131\n",
      "     neutral       0.79      0.79      0.79       136\n",
      "    positive       0.90      0.90      0.90       143\n",
      "\n",
      "    accuracy                           0.83       410\n",
      "   macro avg       0.83      0.83      0.83       410\n",
      "weighted avg       0.83      0.83      0.83       410\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "[[106  18   7]\n",
      " [ 22 107   7]\n",
      " [  4  11 128]]\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 6. Prediction\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nüîé Running prediction on test data...\")\n",
    "\n",
    "# Generate predictions using the trained model\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Convert predictions to Pandas DataFrame for evaluation\n",
    "pdf = predictions.select(\"label_index\", \"prediction\").toPandas()\n",
    "\n",
    "# Ground truth and predictions\n",
    "y_true = pdf[\"label_index\"].astype(int)\n",
    "y_pred = pdf[\"prediction\"].astype(int)\n",
    "\n",
    "# Optional: Map label indices to class names\n",
    "label_names = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "y_true_named = [label_names[i] for i in y_true]\n",
    "y_pred_named = [label_names[i] for i in y_pred]\n",
    "\n",
    "# Evaluate performance using classification report and confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "report = classification_report(y_true_named, y_pred_named, digits=2)\n",
    "conf_matrix = confusion_matrix(y_true_named, y_pred_named, labels=[\"negative\", \"neutral\", \"positive\"])\n",
    "\n",
    "# Print results\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "print(report)\n",
    "\n",
    "print(\"=== Confusion Matrix ===\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c585001f-4c99-44ec-9402-03a87ed35cf0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
